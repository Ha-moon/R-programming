# 텍스트 마이닝 전처리 #카톡에 의미없는 ㅋㅋ ㅠㅠ이런거 지우기...
result$lyric %>%
  #소문자 영어 지우기
  str_remove_all(pattern = '[a-z]')%>%
  #대문자 영어 지우기
  str_remove_all(pattern = '[A-Z]')%>%
  #ㅋㅋㅋ ㅠㅠㅠ ㅎㅎㅎ 지우기
  str_remove_all(pattern = '[ㄱ-ㅣ]')%>%
  #숫자 지우기
  str_remove_all(pattern = '[0-9]')%>%
  #각종 특수기호 지우기
  str_remove_all(pattern = '[~!@#$%^&*()_+\\n\;]')%>% #\는 다른거로 넘어갈때
  #blank지우기
  str_remove_all(pattern = '[ ]')%>%
  #양옆 공백 제거
  str_trim() %>%


# 전처리를 거친 결과 아무것도 남지 않은 문장 지우기
library(dplyr)
result<-result %>%
  dplyr::filter(lyric !="") #공백이 아닌사람만 result에 넣어줌


# 형태소 분석  
result$lyric[1] %>% r_parser_r(language = 'ko')
parsed<-result$lyric %>% r_parser_r(language = 'ko')

# 확인
parsed[50]


# 말뭉치(자료구조)를 생성
corpus<-parsed %>% VectorSource() %>%VCorpus()



# 각각의 문서 확인
# corpus는 형태소 분석에 사용된 문서를 개별 원소로 갖는 리스트
# [[ ]] : list자료구조 표시
inspect(corpus)
inspcet(corpus[1])



# 단어 문사 행렬 만들기
# 행에는 문서 / 열에는 단어
# 2음절 이상인 단어만 남기는 것으로 설정하였습니다.
DocumentTermMatrix(x=corpus,
                   control = list(wordLengths=c(2,Inf),
                                  stopwords=c('응','네','넵')))



# 차원을 확인합니다.
dim(dtm)
inspect(dtm)



# 단어의 수가 많으므로 term-frequency가 희박한(sparse) 컬럼을 제거하는 방식으로 
# dtm의 차원을 줄입니다. 
# sparse의 인자가 작을수록 dtm의 열 개수가 크게 줄어드는 효과가 있습니다 



# 차원을 확인합니다.
dtm_remove<-removeSparseTerms(x=dtm, sparse = as.numeric=0.98)
