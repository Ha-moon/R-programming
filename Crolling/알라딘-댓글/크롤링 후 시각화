# 알라딘 영화 리뷰 100개를 수집

# 실행할때 아래와같은 에러가 발생할 경우에는 
# read_html() -> read_html(options = "HUGE") 이렇게 안에 options = "HUGE"를 줘야합니다! 
# Error in doc_parse_raw(x, encoding = encoding, base_url = base_url, as_html = as_html,  : 
# Excessive depth in document: 256 use XML_PARSE_HUGE option


# (1) 가장 많이 쓰이는 단어 10개 막대그래프 시각화
# (2) wordcloud2 시각화


library(httr)
library(urltools)
library(rvest)
library(jsonlite)
library(dplyr)
library(stringr)


myUA <- 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'

result<-data.frame()
'https://movie.naver.com/movie/bi/mi/review.nhn?code=163788'
'/movie/bi/mi/review.nhn?code=163788'
'/movie/bi/mi/review.nhn?code=163788&page=2'

for (i in 1:10){ #댓글 페이지 번호
  for(j in 1:10){ #댓글
    res <- GET(url = str_c('https://movie.naver.com/movie/bi/mi/review.nhn?code=163788
                           &page=',code[i]),
               query = list(page = j),
               user_agent(myUA))
    
    df<-res %>% content(as = 'text') %>% fromJSON()
    
    temp<-data.frame(title = str_c('알라딘 댓글 페이지',i),
                     review = df$comments$text)
    
    result <-rbind(result,temp)
    cat('현재 ',i,'페이지, 댓글 ', 10*j, '개 수집완료 \n')
    
    Sys.sleep(0.5)
  }
}

View(result)

#############################
library(rJava)


# 텍스트 마이닝 시각화
install.packages("wordcloud2")
install.packages("tm")
library(wordcloud2)
library(tm)

# R에서 다양한 색깔을 선택할 수 있게 해주는 라이브러리
library(RColorBrewer)


# 텍스트 마이닝 패키지
install.packages('C:/Program Files/NLP4kec~~~',
                 repos = NULL)
library(NLP4kec)


result$lyric %>%
  str_remove_all(pattern = '[a-z]')%>%
  str_remove_all(pattern = '[A-Z]')%>%
  str_remove_all(pattern = '[ㄱ-ㅣ]')%>%
  str_remove_all(pattern = '[0-9]')%>%
  str_remove_all(pattern = '[~!@#$%^&*()_+.\\n\;🦁🐺🦌🐉]❤️')%>% #
  str_trim() %>%
  
  
  library(dplyr)
result<-result %>%
  dplyr::filter(review !='')


# 형태소 분석  
parse<-r_parser_r(contentVector = result$review, language = 'ko')

# 말뭉치(자료구조)를 생성
corpus<-parsed %>% VectorSource() %>%VCorpus()


# 각각의 문서 확인
inspcet(corpus[1])

# 단어 문사 행렬 만들기
dtm<-DocumentTermMatrix(x=corpus,
                        control = list(wordLengths = c(2,Inf),
                                       stopwords=c('응','네','넵')))

# 차원을 확인합니다.
dim(dtm)
inspect(dtm)

dtm_remove<-removeSparseTerms(x=dtm, sparse=0.99)


# 차원을 확인합니다.
dim(dtm_remove)

# 단어 문서 행렬 확인
inspect(dtm_remove)

# 행렬로 변환
mat<-as.matrix(dtm_remove)

# 동사 제거
colnames(mat)
mat2<-mat[, colnames(mat) %>% str_sub(-1) !="다"]


# 열을 기준으로 단어의 수 확인
# 참고)apply(자료구조, 1(행) or 2(열), 사용할 함수)
word <- apply(mat2, 2, sum) %>% sort(decreasing = TRUE)


# data.frame()으로 변환
df<-data.frame(word = names(word), freq=word)


# 행 이름 변경
rownames(df) <-seq(from=1, to=nrow(df), by=1)
df

#막대그래프 그리기
library(ggplot2)

df %>%
  arrange(desc(freq)) %>%
  head(10)%>%
  ggplot(mapping = aes(x=reorder(word,-freq), y=freq, fill=word)) +
  geom_bar(stat= "identity")


# 시각화
wordcloud2(df)
